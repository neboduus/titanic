{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy test: 1.75 should be 1.75\n",
      "Eliminating some features... [0, 3, 8, 10]\n",
      "Trasforming categorical vars to continous...\n",
      "Casual Var:  [1 2 3]\n",
      "Trasforming:  1\n",
      "Trasforming:  2\n",
      "Trasforming:  3\n",
      "Casual Var:  ['female' 'male']\n",
      "Trasforming:  female\n",
      "Trasforming:  male\n",
      "Casual Var:  [0 'C' 'Q' 'S']\n",
      "Trasforming:  0\n",
      "Trasforming:  C\n",
      "Trasforming:  Q\n",
      "Trasforming:  S\n",
      "Transforming DONE\n",
      "New dataset shape (890, 16)\n",
      "Output entropy: 0.176137735296\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-80d5649e58fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'Output entropy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m                         \u001b[0;31m# total entropy of training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m \u001b[0mdecision_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'DecisionTree: '\u001b[0m                                                  \u001b[0;31m# construct the decision tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdecision_tree\u001b[0m\u001b[0;34m)\u001b[0m                                                           \u001b[0;31m# print the tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-80d5649e58fa>\u001b[0m in \u001b[0;36mbuild_tree\u001b[0;34m(D)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# left subtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mH1\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mentropy_threshold\u001b[0m\u001b[0;34m:\u001b[0m                                      \u001b[0;31m# if we found an entropy less than our threshold it means we found a leaf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# we say that the most common value in this leaf is the rappresentant of this subtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m                                                                           \u001b[0;31m# otherwise it means that we have to split again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# My first model for Titanic Problem\n",
    "#\n",
    "# Tring to predict who died on the Titanic and\n",
    "# and who lived throught a decision tree\n",
    "#\n",
    "# Data needed:  train.csv \n",
    "#\t\t\t\ttest.csv\n",
    "#\n",
    "# Esecution:\n",
    "#    python2.7 myTitanicModel.py\n",
    "# or\n",
    "#    chmod 755 myTitanicModel.py   <--- tantum\n",
    "#    ./myTitanicModel.py\n",
    "\n",
    "##################################################\n",
    "#\n",
    "# Modules\n",
    "# Reading and Manipulation of the dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Numerics Methods\n",
    "import numpy as np\n",
    "\n",
    "# Pretty-printing of data structures\n",
    "import pprint\n",
    "\n",
    "# Counting and Enumeration of values (using for calculate the frequency of inputs)\n",
    "import collections\n",
    "\n",
    "import math\n",
    "\n",
    "######################################\n",
    "#\n",
    "# Parameters\n",
    "\n",
    "# Minimum entropy that a dataset need to have to divide it\n",
    "entropy_threshold = 0.001\n",
    "\n",
    "##########################################\n",
    "# Functions\n",
    "\n",
    "# Calculate Entropy: given a vector of objects(numerics or strings),\n",
    "# estimate the entropy of the Casual Variable where it comes from\n",
    "def entropy(y):\n",
    "\tcount = collections.Counter(y) \t\t\t# count how many times each value is present in y\n",
    "\ts = 0\t\t\t\t\t\t\t\t\t# for cumulate the sum\n",
    "\tm = float(len(y))\t\t\t\t\t\t# divisor for calculating frequencies\n",
    "\t\n",
    "\t# now for every value present in the vector\n",
    "\tfor c in count.values():\n",
    "\t\tp = c/m\t\t\t\t\t\t\t\t# estimate the probability of that value\n",
    "\t\ts -= p * np.log2(p)\t\t\t\t\t# add the entropy to the total entropy \n",
    "\t\n",
    "\treturn s\t\t\t\t\t\t\t\t# return the total entropy found\n",
    "\n",
    "# Testing the entropy function: the following call is made on a vector with 4 diffrents values with frequency 1/2, 1/4, 1/8, 1/8\n",
    "# and the function should return entropy = 7/4\n",
    "print 'Entropy test:', entropy([1,'a',1,'pippo',1,'a',4.7,1]), 'should be', 7.0/4.0\n",
    "\n",
    "# Partition of a dataset D in 2 subpartitions d1 and d2 such us to minimize the expected entropy\n",
    "# The partition is based on a condition of the form \"x[j]<theta\", and seek for the optimal values of j and theta\n",
    "# The second parameter is the OUTPUT column which should be excluded\n",
    "def find_split(D):\n",
    "\tH_min = 1.0e100\t\t\t\t\t\t\t# remember the minimum entropy\n",
    "\tm,n = D.shape\t\t\t\t\t\t\t# get the numbers of features and samples\n",
    "\t\n",
    "\tfor j in range(n):\t\t\t\t\t\t# exclude output column\n",
    "\t\tfor theta in np.unique(D[[j]]):\t\t\t# iterate on all possible values of that column\n",
    "\t\t\td1 = D[ D[j]<theta ]\t\t\t# put in d1 the samples for which x[j]<theta and in d2 the remainings\n",
    "\t\t\td2 = D[ D[j]>=theta ]\n",
    "\t\t\t\n",
    "\t\t\tH1 = entropy(d1[n-1])\t\t\t# calculate entropy of the splitting (excluding the output column)\n",
    "\t\t\tH2 = entropy(d2[n-1])\n",
    "\t\t\t\n",
    "\t\t\tH_mean = ( (H1*len(d1)) + (H2*len(d2)) ) / m\t# calculate the Average Entropy\n",
    "\t\t\t\n",
    "\t\t\tif H_mean < H_min :\t\t\t\t# if we found a lower entropy we store the minimum parameters and minimum entropy\n",
    "\t\t\t\tH_min = H_mean\n",
    "\t\t\t\tj_min = j\n",
    "\t\t\t\ttheta_min = theta\n",
    "\t\t\t\td1_min = d1\n",
    "\t\t\t\td2_min = d2\n",
    "\t\t\t\tH1_min = H1\n",
    "\t\t\t\tH2_min = H2\n",
    "\treturn j_min, theta_min, d1_min, d2_min, H1_min, H2_min\t\t#return the minimum parameters\n",
    "\t\n",
    "# Recursive construction of the tree starting from dataset D\n",
    "# We indicate with y the output feature\n",
    "def build_tree(D):\n",
    "\tm, n = D.shape\t\t\t\t\t\t\t\t# get the number of features and samples\n",
    "\tj, theta, d1, d2, H1, H2 = find_split(D)\t# find the ottimal parameters for constructing the tree\n",
    "\t\n",
    "\t#print 'x[',j,'] <',theta\n",
    "\t#print 'H1, H2 =', H1, H2\n",
    "\t#print 'd1 output = ', d1[[11]]\n",
    "\t#print 'd2 output =', d2[[11]]\n",
    "\n",
    "\t# left subtree\n",
    "\tif H1 < entropy_threshold:\t\t\t\t\t# if we found an entropy less than our threshold it means we found a leaf\n",
    "\t\tleft = collections.Counter(d1[n-1]).most_common()[0][0]\t# we say that the most common value in this leaf is the rappresentant of this subtree\n",
    "\telse:\t\t\t\t\t\t\t\t\t\t# otherwise it means that we have to split again\n",
    "\t\tleft = build_tree(d1)\n",
    "\t\t\n",
    "\t# right subtree (as for the left one)\n",
    "\tif H2 < entropy_threshold:\n",
    "\t\tright = collections.Counter(d2[n-1]).most_common(1)[0][0]\n",
    "\telse:\n",
    "\t\tright = build_tree(d2)\n",
    "\t\n",
    "\treturn j, theta, left, right\n",
    "\t\n",
    "# Classifing a vector of features x, using the tree 'tree', indicating the output feature y\n",
    "# Note that the function is tail_recursive (we can write it using a while cycle whithout using stacks)\n",
    "def classify(x,y,tree):\n",
    "\tif isinstance(tree, tuple):\t\t\t\t\t# if the tree is a tuple it means that it isn't a leaf so we have to visit it deeper\n",
    "\t\tj, theta, left, right = tree\t\t\t# we extract the parameters\n",
    "\t\treturn classify(x, y, left if x[j]<theta else right)\t# we call recursively classify(...) on the left or on the right tree based on the condition 'x[j]<theta'\n",
    "\telse:\n",
    "\t\treturn tree\t\t\t\t\t\t\t\t# if the tree is a leaf it means that it is contains the value to be predicted\n",
    "\n",
    "# Moving the 2nd column(which is the output to predict) at the end of the DataFrame\n",
    "def move_output_to_end(D, col):\n",
    "    m, n = D.shape\n",
    "    cols = range(n)\n",
    "\t\n",
    "    if col>=n or col<0:\n",
    "        return\n",
    "    if col is 0:\n",
    "        cols = cols[1:n] + cols[0:1]\n",
    "    else:\n",
    "        cols = cols[0:col] + cols[col+1:n+1] + cols[col:col+1]\n",
    "    \n",
    "    return pd.DataFrame(D)\n",
    "\n",
    "# Trasforming categoric features into continous\n",
    "def categoric_to_continuous(D):\n",
    "\tm,n = D.shape\n",
    "\tindex1 = -1\n",
    "\n",
    "\tconCols = [1, 5, 6, 7, 9]\t\n",
    "\tcatCols = [2, 4, 11]\n",
    "\toutCols = [0, 3, 8, 10]\n",
    "\t\n",
    "\tprint 'Eliminating some features...', outCols\n",
    "\n",
    "\tD = np.array(D)\n",
    "\t#brutal transforming NaN in 0(zeros)\n",
    "\tfor x in range(m):\n",
    "\t\tfor y in range(n):\n",
    "\t\t\tif pd.isnull(D[x,y]):\n",
    "\t\t\t\tD[x,y]=0\n",
    "\tD = pd.DataFrame(D)\n",
    "\n",
    "\tnewDataFrame = D[conCols]\t\n",
    "\toldDataFrame = D[catCols]\n",
    "\told = np.array(oldDataFrame)\n",
    "\tcontinouzedFrame = np.array(np.zeros((m,11)))\n",
    "\tD = np.array(D)\n",
    "\t\n",
    "\tnewDataFrame = np.array(newDataFrame)\n",
    "\n",
    "\tprint 'Trasforming categorical vars to continous...'\n",
    "\tfor i in catCols:\n",
    "\t\tvalues = np.unique(D[:,i])\n",
    "\t\tprint 'Casual Var: ', values\n",
    "\t\tfor val in values:\n",
    "\t\t\tindex1 += 1\n",
    "\t\t\tprint 'Trasforming: ', val\n",
    "\t\t\tfor j in range(m):\n",
    "\t\t\t\tif D[j,i] is val:\n",
    "\t\t\t\t\tcontinouzedFrame[j,index1] = 1 \n",
    "\n",
    "\t\n",
    "\tnew = pd.DataFrame(np.hstack((newDataFrame, continouzedFrame)))\n",
    "\tprint 'Transforming DONE'\n",
    "\tprint 'New dataset shape', new.shape\n",
    "\treturn new\n",
    "\t\n",
    "\t\n",
    "\n",
    "###################################################\n",
    "#\n",
    "# Main Program\n",
    "#\n",
    "# Training Set Reading\n",
    "train = pd.read_csv('train.csv', header=1)\n",
    "# Test Set Reading\n",
    "test = pd.read_csv('test.csv', header=1)\t\t\t\n",
    "\t\t\n",
    "train = categoric_to_continuous(train)\n",
    "train = move_output_to_end(train, 0)\n",
    "\n",
    "m,n = train.shape\n",
    "\n",
    "#print 'Test: ', test\n",
    "\n",
    "#prediction_file = open(\"firstGenderModel.csv\", \"wb\")\t\t#open a pointer to a new file so we can write the predicted results on it\n",
    "#prediction_file_object = csv.writer(prediction_file)\n",
    "\n",
    "m, n = train.shape\n",
    "print 'Output entropy:', entropy(train)\t\t\t\t# total entropy of training set\n",
    "\n",
    "decision_tree = build_tree(train)\n",
    "print 'DecisionTree: '\t\t\t\t\t\t\t# construct the decision tree\n",
    "pprint.pprint (decision_tree)\t\t\t\t\t\t\t\t# print the tree\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
